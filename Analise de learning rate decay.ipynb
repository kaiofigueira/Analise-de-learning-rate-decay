{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCoycU-0L579"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WKev1b1OA9C",
    "outputId": "6868c3a0-3e32-43bd-da35-013f90f40b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_MJpqPTOJw_"
   },
   "outputs": [],
   "source": [
    "amostra_treino = np.random.choice(range(len(x_train)), size=30000, replace=False)\n",
    "amostra_teste = np.random.choice(range(len(x_test)), size=3000, replace=False)\n",
    "\n",
    "x_train = x_train[amostra_treino,:]\n",
    "y_train = y_train[amostra_treino,:]\n",
    "\n",
    "x_test = x_test[amostra_teste,:]\n",
    "y_test = y_test[amostra_teste,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwWk_byPON-B",
    "outputId": "4cb8896c-fc9d-4458-9c7f-70c366910b07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27358,  2734, 24384, ...,  1171, 15789,   822])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amostra_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PTqZl4TOOHE"
   },
   "outputs": [],
   "source": [
    "def esqueci_as_classes_me_ajuda(pred):\n",
    "    dicionario = {0: \"airplane\",\n",
    "           1: \"automobile\",\n",
    "           2: \"bird\",\n",
    "           3: \"cat\",\n",
    "           4: \"deer\",\n",
    "           5: \"dog\",\n",
    "           6: \"frog\",\n",
    "           7: \"horse\",\n",
    "           8: \"ship\",\n",
    "           9: \"truck\"}\n",
    "    return dicionario[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a distribuição dos dados de Treino e Test, pois a cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "FeOrsHuTOOJ7",
    "outputId": "2f6605c5-1908-4700-b9e5-4c3b72c10115"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2977., 3032., 3004., 2997., 2985., 2992., 3007., 2993., 3013.,\n",
       "        3000.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, rwidth = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "2jYhy8ASOOM8",
    "outputId": "5237aef5-f719-4399-cd06-0e1c8f871dbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([291., 307., 323., 314., 290., 303., 288., 271., 286., 327.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test, rwidth = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbQibb0AOYda"
   },
   "outputs": [],
   "source": [
    "# one hot encoding nos y\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando um dado aleatorio do conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "KPODMPyPOYgN",
    "outputId": "a89fde16-d8f6-4954-bd1c-e476662bd9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(low=0, high=3000)\n",
    "y_test[i]\n",
    "plt.imshow(x_test[i])\n",
    "print(y_test[i])\n",
    "print(esqueci_as_classes_me_ajuda(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "herlBiioPjrD",
    "outputId": "bd107fa0-0ea6-4113-dff7-025ba4291636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes (base é a imagenet)\n",
    "# Aqui vamos usar para as imagens 32 x 32 do CIFAR10\n",
    "\n",
    "# Loads the VGG16 model without the top or FC layers\n",
    "vgg16_model = vgg16.VGG16(weights='imagenet', include_top = False, input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiando a estrutura da rede neural da vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3nMskN5PspJ"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mJCFqbmPyBB",
    "outputId": "8875cb67-1071-43de-8047-d9cc4d543f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6isEQlsYP1LW"
   },
   "outputs": [],
   "source": [
    "# Congelando os pesos que não vou treinar\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3PceRk5P1T9",
    "outputId": "130c0d5c-a291-45ca-e6ae-bcd1d1231ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwXfMam-P1Y2"
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGV4qM0oZYIx"
   },
   "source": [
    "Adicionei o batch normalization, com o intuito de deixar o treinamento mais rapido e deixar o grafico da loss mais estavel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BlMY2l9P1eY",
    "outputId": "85968c5a-9fa1-4121-e3a9-9861ac787cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                16416     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,733,482\n",
      "Trainable params: 17,770\n",
      "Non-trainable params: 14,715,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ac09DLe8P1hW",
    "outputId": "1a714e4a-7f21-4760-f849-f03d18b5f58d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'dense_1')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um stopper que se nao tiver alteração em 10 epocas, entao o callstopper sera disparado e parará o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B82oL0zEP1kH"
   },
   "outputs": [],
   "source": [
    "# callbacks: early stopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z83onlYtYSAV"
   },
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_lea0000000000000000000000000000000000000000000000rning_rate=1e-2,\n",
    "    decay_steps = 10000,0 \n",
    "    decay_rate = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3S2IbJIP1nW"
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlUjgN1HTHMO"
   },
   "outputs": [],
   "source": [
    "# Salvando os pesos antes de treinar.\n",
    "#model.save_weights('pesos_iniciais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFvBLf-4UlCh"
   },
   "outputs": [],
   "source": [
    "# pré-processamento das imagens assim como pede na documentação da VGG\n",
    "input_test = tf.keras.applications.vgg16.preprocess_input(x_test)\n",
    "input_train = tf.keras.applications.vgg16.preprocess_input(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEjzZMaNazt1",
    "outputId": "27769593-4de7-4304-c473-39802bab745e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f577079a220>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_weights('pesos_iniciais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K68C7tBOUpI4",
    "outputId": "543fcb42-5592-4fa3-ad13-dbda5e810d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.9489 - accuracy: 0.3223 - val_loss: 1.6251 - val_accuracy: 0.4363\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4771 - accuracy: 0.4942 - val_loss: 1.4148 - val_accuracy: 0.5100\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3260 - accuracy: 0.5414 - val_loss: 1.3187 - val_accuracy: 0.5450\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.2444 - accuracy: 0.5724 - val_loss: 1.2608 - val_accuracy: 0.5617\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.1863 - accuracy: 0.5876 - val_loss: 1.2228 - val_accuracy: 0.5740\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.1416 - accuracy: 0.6039 - val_loss: 1.1930 - val_accuracy: 0.5793\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.1158 - accuracy: 0.6105 - val_loss: 1.1694 - val_accuracy: 0.5887\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.0909 - accuracy: 0.6212 - val_loss: 1.1508 - val_accuracy: 0.5917\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.0665 - accuracy: 0.6296 - val_loss: 1.1352 - val_accuracy: 0.5960\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.0492 - accuracy: 0.6349 - val_loss: 1.1232 - val_accuracy: 0.6020\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.0323 - accuracy: 0.6405 - val_loss: 1.1121 - val_accuracy: 0.6020\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.0168 - accuracy: 0.6455 - val_loss: 1.1041 - val_accuracy: 0.6063\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.0029 - accuracy: 0.6511 - val_loss: 1.0962 - val_accuracy: 0.6117\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9954 - accuracy: 0.6534 - val_loss: 1.0901 - val_accuracy: 0.6153\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.9833 - accuracy: 0.6560 - val_loss: 1.0829 - val_accuracy: 0.6197\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.9737 - accuracy: 0.6564 - val_loss: 1.0781 - val_accuracy: 0.6227\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.9643 - accuracy: 0.6633 - val_loss: 1.0737 - val_accuracy: 0.6240\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.9583 - accuracy: 0.6657 - val_loss: 1.0699 - val_accuracy: 0.6220\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9507 - accuracy: 0.6675 - val_loss: 1.0662 - val_accuracy: 0.6287\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.9414 - accuracy: 0.6707 - val_loss: 1.0617 - val_accuracy: 0.6303\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9343 - accuracy: 0.6732 - val_loss: 1.0598 - val_accuracy: 0.6307\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9299 - accuracy: 0.6731 - val_loss: 1.0574 - val_accuracy: 0.6320\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.9229 - accuracy: 0.6779 - val_loss: 1.0549 - val_accuracy: 0.6323\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9204 - accuracy: 0.6768 - val_loss: 1.0536 - val_accuracy: 0.6333\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.9127 - accuracy: 0.6811 - val_loss: 1.0516 - val_accuracy: 0.6347\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.9092 - accuracy: 0.6817 - val_loss: 1.0502 - val_accuracy: 0.6347\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.9017 - accuracy: 0.6857 - val_loss: 1.0479 - val_accuracy: 0.6367\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8986 - accuracy: 0.6855 - val_loss: 1.0463 - val_accuracy: 0.6410\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8931 - accuracy: 0.6857 - val_loss: 1.0465 - val_accuracy: 0.6350\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8884 - accuracy: 0.6906 - val_loss: 1.0445 - val_accuracy: 0.6380\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8863 - accuracy: 0.6910 - val_loss: 1.0425 - val_accuracy: 0.6390\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8807 - accuracy: 0.6934 - val_loss: 1.0419 - val_accuracy: 0.6430\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8801 - accuracy: 0.6921 - val_loss: 1.0408 - val_accuracy: 0.6430\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8741 - accuracy: 0.6940 - val_loss: 1.0409 - val_accuracy: 0.6393\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8723 - accuracy: 0.6950 - val_loss: 1.0389 - val_accuracy: 0.6437\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8701 - accuracy: 0.6960 - val_loss: 1.0397 - val_accuracy: 0.6407\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8660 - accuracy: 0.6968 - val_loss: 1.0395 - val_accuracy: 0.6430\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8633 - accuracy: 0.6973 - val_loss: 1.0399 - val_accuracy: 0.6407\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8577 - accuracy: 0.6979 - val_loss: 1.0396 - val_accuracy: 0.6423\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8557 - accuracy: 0.7013 - val_loss: 1.0402 - val_accuracy: 0.6440\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8533 - accuracy: 0.7019 - val_loss: 1.0389 - val_accuracy: 0.6447\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8490 - accuracy: 0.7016 - val_loss: 1.0392 - val_accuracy: 0.6440\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8484 - accuracy: 0.7036 - val_loss: 1.0379 - val_accuracy: 0.6443\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8448 - accuracy: 0.7037 - val_loss: 1.0374 - val_accuracy: 0.6447\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8387 - accuracy: 0.7075 - val_loss: 1.0373 - val_accuracy: 0.6453\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8427 - accuracy: 0.7051 - val_loss: 1.0380 - val_accuracy: 0.6463\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8348 - accuracy: 0.7066 - val_loss: 1.0378 - val_accuracy: 0.6430\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8348 - accuracy: 0.7069 - val_loss: 1.0383 - val_accuracy: 0.6450\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8336 - accuracy: 0.7075 - val_loss: 1.0391 - val_accuracy: 0.6470\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8297 - accuracy: 0.7089 - val_loss: 1.0386 - val_accuracy: 0.6460\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8290 - accuracy: 0.7083 - val_loss: 1.0385 - val_accuracy: 0.6443\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8290 - accuracy: 0.7095 - val_loss: 1.0416 - val_accuracy: 0.6430\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8231 - accuracy: 0.7132 - val_loss: 1.0392 - val_accuracy: 0.6467\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8253 - accuracy: 0.7128 - val_loss: 1.0409 - val_accuracy: 0.6453\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8195 - accuracy: 0.7120 - val_loss: 1.0395 - val_accuracy: 0.6473\n",
      "Epoch 55: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57c85d00a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_train,\n",
    "           y_train,\n",
    "           epochs=100,\n",
    "           batch_size=128,\n",
    "           validation_data=(input_test, y_test)\n",
    "           , callbacks=[stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkkwfQwvhMKY"
   },
   "source": [
    "Adicionei um Early stopper para monitorar o val_loss e para ser disparado depois de 10 epocas sem grandes mudanças. O resultado foi que não teve grandes mudanças na val_loss da epoca 45 a 55, ficando em 1.04. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFCmCJUyYQqc"
   },
   "outputs": [],
   "source": [
    "boundaries = [1000, 1100]\n",
    "values = [1e-2, 1e-1, 1e-2]\n",
    "\n",
    "lr_schedule_pwcd = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries, values, name=None\n",
    ")\n",
    "\n",
    "# compile model\n",
    "opt = keras.optimizers.SGD(learning_rate=lr_schedule_pwcd)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToCXdBVTg4yk",
    "outputId": "38b92685-c774-48a2-ec78-b22d1796b9d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f57c85eebe0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pesos_iniciais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsdhQ7uog-dy",
    "outputId": "041d5e14-09ac-48dc-d3f8-c8fcf73f9a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 7s 23ms/step - loss: 1.9488 - accuracy: 0.3253 - val_loss: 1.6262 - val_accuracy: 0.4350\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4763 - accuracy: 0.4979 - val_loss: 1.4158 - val_accuracy: 0.5130\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3244 - accuracy: 0.5479 - val_loss: 1.3180 - val_accuracy: 0.5460\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.2421 - accuracy: 0.5720 - val_loss: 1.2594 - val_accuracy: 0.5647\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.1469 - accuracy: 0.6000 - val_loss: 1.1468 - val_accuracy: 0.5973\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.0559 - accuracy: 0.6338 - val_loss: 1.1311 - val_accuracy: 0.6007\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.0392 - accuracy: 0.6375 - val_loss: 1.1201 - val_accuracy: 0.6067\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.0246 - accuracy: 0.6428 - val_loss: 1.1092 - val_accuracy: 0.6080\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.0094 - accuracy: 0.6477 - val_loss: 1.0992 - val_accuracy: 0.6127\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.9960 - accuracy: 0.6531 - val_loss: 1.0911 - val_accuracy: 0.6190\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9853 - accuracy: 0.6562 - val_loss: 1.0858 - val_accuracy: 0.6190\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.9766 - accuracy: 0.6592 - val_loss: 1.0799 - val_accuracy: 0.6217\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.9659 - accuracy: 0.6624 - val_loss: 1.0760 - val_accuracy: 0.6243\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.9594 - accuracy: 0.6654 - val_loss: 1.0709 - val_accuracy: 0.6250\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.9498 - accuracy: 0.6691 - val_loss: 1.0675 - val_accuracy: 0.6277\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.9433 - accuracy: 0.6702 - val_loss: 1.0626 - val_accuracy: 0.6257\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9359 - accuracy: 0.6718 - val_loss: 1.0599 - val_accuracy: 0.6300\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.9261 - accuracy: 0.6763 - val_loss: 1.0583 - val_accuracy: 0.6357\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.9241 - accuracy: 0.6752 - val_loss: 1.0565 - val_accuracy: 0.6313\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.9180 - accuracy: 0.6802 - val_loss: 1.0547 - val_accuracy: 0.6323\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.9150 - accuracy: 0.6789 - val_loss: 1.0515 - val_accuracy: 0.6380\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.9082 - accuracy: 0.6817 - val_loss: 1.0488 - val_accuracy: 0.6340\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.9006 - accuracy: 0.6842 - val_loss: 1.0478 - val_accuracy: 0.6357\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8994 - accuracy: 0.6859 - val_loss: 1.0453 - val_accuracy: 0.6363\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.8937 - accuracy: 0.6867 - val_loss: 1.0448 - val_accuracy: 0.6353\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8882 - accuracy: 0.6904 - val_loss: 1.0448 - val_accuracy: 0.6363\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8861 - accuracy: 0.6897 - val_loss: 1.0441 - val_accuracy: 0.6407\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8805 - accuracy: 0.6924 - val_loss: 1.0435 - val_accuracy: 0.6407\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8768 - accuracy: 0.6936 - val_loss: 1.0403 - val_accuracy: 0.6413\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.8736 - accuracy: 0.6942 - val_loss: 1.0415 - val_accuracy: 0.6410\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8665 - accuracy: 0.6963 - val_loss: 1.0399 - val_accuracy: 0.6427\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8697 - accuracy: 0.6967 - val_loss: 1.0379 - val_accuracy: 0.6417\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8654 - accuracy: 0.6991 - val_loss: 1.0387 - val_accuracy: 0.6423\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8565 - accuracy: 0.6992 - val_loss: 1.0380 - val_accuracy: 0.6420\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8557 - accuracy: 0.7033 - val_loss: 1.0379 - val_accuracy: 0.6450\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8546 - accuracy: 0.6999 - val_loss: 1.0385 - val_accuracy: 0.6453\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8500 - accuracy: 0.7018 - val_loss: 1.0373 - val_accuracy: 0.6437\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8444 - accuracy: 0.7037 - val_loss: 1.0371 - val_accuracy: 0.6447\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8440 - accuracy: 0.7031 - val_loss: 1.0380 - val_accuracy: 0.6447\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8409 - accuracy: 0.7062 - val_loss: 1.0398 - val_accuracy: 0.6433\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8384 - accuracy: 0.7065 - val_loss: 1.0380 - val_accuracy: 0.6440\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.8365 - accuracy: 0.7074 - val_loss: 1.0390 - val_accuracy: 0.6433\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8309 - accuracy: 0.7081 - val_loss: 1.0399 - val_accuracy: 0.6437\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 0.8299 - accuracy: 0.7110 - val_loss: 1.0378 - val_accuracy: 0.6437\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8268 - accuracy: 0.7109 - val_loss: 1.0403 - val_accuracy: 0.6447\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8254 - accuracy: 0.7101 - val_loss: 1.0398 - val_accuracy: 0.6447\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.8229 - accuracy: 0.7107 - val_loss: 1.0399 - val_accuracy: 0.6443\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.8217 - accuracy: 0.7097 - val_loss: 1.0396 - val_accuracy: 0.6427\n",
      "Epoch 48: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f58583b04f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_train,\n",
    "           y_train,\n",
    "           epochs=100,\n",
    "           batch_size=128,\n",
    "           validation_data=(input_test, y_test)\n",
    "           , callbacks=[stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_RzmQPGhED9"
   },
   "source": [
    "Early stopper parou na epoca 48, diferente do outro algoritmo de decaimento de learning rate.\n",
    "\n",
    "Resultados: \n",
    "\n",
    "1- O PiecewiseConstantDecay foi um pouquinho de nada mais lento que o ExponentialDecay no tempo de execução individual a cada epoca.\n",
    "            \n",
    "2- O PiecewiseConstantDecay convergiu mais rapido(epoca 48) do que o ExponentialDecay(epoca 55).\n",
    "\n",
    "3- val_loss = 1.04 em ambos os algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMPr5gSThERL"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-2\n",
    "decay_steps = 100.0\n",
    "decay_rate = 0.9\n",
    "learning_rate_ITD = keras.optimizers.schedules.InverseTimeDecay(\n",
    "  initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "\n",
    "# compile model\n",
    "opt = keras.optimizers.SGD(learning_rate=learning_rate_ITD)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxvGvxKLiITL",
    "outputId": "0353950c-89d8-449d-c71b-b50ac37a1c80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f5770728d30>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pesos_iniciais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0x02m9MVjWA5",
    "outputId": "d50459ae-8efc-452d-ec63-c23ce676c8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 2.0766 - accuracy: 0.2776 - val_loss: 1.8515 - val_accuracy: 0.3550\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.7509 - accuracy: 0.3954 - val_loss: 1.7137 - val_accuracy: 0.4033\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6490 - accuracy: 0.4357 - val_loss: 1.6485 - val_accuracy: 0.4263\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.5984 - accuracy: 0.4512 - val_loss: 1.6080 - val_accuracy: 0.4430\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.5608 - accuracy: 0.4662 - val_loss: 1.5795 - val_accuracy: 0.4513\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5358 - accuracy: 0.4741 - val_loss: 1.5578 - val_accuracy: 0.4587\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5175 - accuracy: 0.4821 - val_loss: 1.5416 - val_accuracy: 0.4667\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.5027 - accuracy: 0.4862 - val_loss: 1.5280 - val_accuracy: 0.4720\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4883 - accuracy: 0.4901 - val_loss: 1.5156 - val_accuracy: 0.4780\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.4782 - accuracy: 0.4946 - val_loss: 1.5058 - val_accuracy: 0.4803\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4660 - accuracy: 0.4977 - val_loss: 1.4955 - val_accuracy: 0.4837\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.4573 - accuracy: 0.5011 - val_loss: 1.4886 - val_accuracy: 0.4867\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4501 - accuracy: 0.5045 - val_loss: 1.4814 - val_accuracy: 0.4887\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4405 - accuracy: 0.5070 - val_loss: 1.4745 - val_accuracy: 0.4907\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.4373 - accuracy: 0.5068 - val_loss: 1.4697 - val_accuracy: 0.4923\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4289 - accuracy: 0.5121 - val_loss: 1.4643 - val_accuracy: 0.4940\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4268 - accuracy: 0.5119 - val_loss: 1.4589 - val_accuracy: 0.4953\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4215 - accuracy: 0.5139 - val_loss: 1.4544 - val_accuracy: 0.4963\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4142 - accuracy: 0.5171 - val_loss: 1.4510 - val_accuracy: 0.4980\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 1.4154 - accuracy: 0.5133 - val_loss: 1.4481 - val_accuracy: 0.4980\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4097 - accuracy: 0.5179 - val_loss: 1.4438 - val_accuracy: 0.5017\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4045 - accuracy: 0.5205 - val_loss: 1.4409 - val_accuracy: 0.5027\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.4019 - accuracy: 0.5178 - val_loss: 1.4378 - val_accuracy: 0.5047\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3972 - accuracy: 0.5202 - val_loss: 1.4345 - val_accuracy: 0.5040\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3942 - accuracy: 0.5222 - val_loss: 1.4315 - val_accuracy: 0.5060\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3931 - accuracy: 0.5232 - val_loss: 1.4295 - val_accuracy: 0.5070\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3873 - accuracy: 0.5254 - val_loss: 1.4264 - val_accuracy: 0.5090\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.3873 - accuracy: 0.5250 - val_loss: 1.4243 - val_accuracy: 0.5077\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3848 - accuracy: 0.5238 - val_loss: 1.4230 - val_accuracy: 0.5100\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3835 - accuracy: 0.5258 - val_loss: 1.4197 - val_accuracy: 0.5090\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3816 - accuracy: 0.5271 - val_loss: 1.4172 - val_accuracy: 0.5100\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3793 - accuracy: 0.5269 - val_loss: 1.4161 - val_accuracy: 0.5120\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3732 - accuracy: 0.5291 - val_loss: 1.4143 - val_accuracy: 0.5133\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3738 - accuracy: 0.5282 - val_loss: 1.4116 - val_accuracy: 0.5127\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3716 - accuracy: 0.5287 - val_loss: 1.4098 - val_accuracy: 0.5137\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3688 - accuracy: 0.5311 - val_loss: 1.4083 - val_accuracy: 0.5153\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3673 - accuracy: 0.5302 - val_loss: 1.4077 - val_accuracy: 0.5163\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3659 - accuracy: 0.5311 - val_loss: 1.4050 - val_accuracy: 0.5143\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3629 - accuracy: 0.5324 - val_loss: 1.4044 - val_accuracy: 0.5153\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3669 - accuracy: 0.5329 - val_loss: 1.4030 - val_accuracy: 0.5160\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3614 - accuracy: 0.5332 - val_loss: 1.4010 - val_accuracy: 0.5153\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3582 - accuracy: 0.5334 - val_loss: 1.4003 - val_accuracy: 0.5167\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3577 - accuracy: 0.5330 - val_loss: 1.3986 - val_accuracy: 0.5153\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3579 - accuracy: 0.5352 - val_loss: 1.3968 - val_accuracy: 0.5193\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3559 - accuracy: 0.5366 - val_loss: 1.3961 - val_accuracy: 0.5200\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3547 - accuracy: 0.5329 - val_loss: 1.3959 - val_accuracy: 0.5197\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3516 - accuracy: 0.5354 - val_loss: 1.3933 - val_accuracy: 0.5197\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3513 - accuracy: 0.5385 - val_loss: 1.3927 - val_accuracy: 0.5197\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3515 - accuracy: 0.5376 - val_loss: 1.3921 - val_accuracy: 0.5207\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3506 - accuracy: 0.5358 - val_loss: 1.3902 - val_accuracy: 0.5197\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3467 - accuracy: 0.5384 - val_loss: 1.3888 - val_accuracy: 0.5223\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3454 - accuracy: 0.5385 - val_loss: 1.3878 - val_accuracy: 0.5230\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3456 - accuracy: 0.5380 - val_loss: 1.3869 - val_accuracy: 0.5217\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3439 - accuracy: 0.5370 - val_loss: 1.3861 - val_accuracy: 0.5217\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3429 - accuracy: 0.5386 - val_loss: 1.3858 - val_accuracy: 0.5233\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3420 - accuracy: 0.5406 - val_loss: 1.3843 - val_accuracy: 0.5220\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3429 - accuracy: 0.5396 - val_loss: 1.3833 - val_accuracy: 0.5223\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3417 - accuracy: 0.5372 - val_loss: 1.3816 - val_accuracy: 0.5273\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3391 - accuracy: 0.5392 - val_loss: 1.3811 - val_accuracy: 0.5260\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3393 - accuracy: 0.5405 - val_loss: 1.3811 - val_accuracy: 0.5250\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3380 - accuracy: 0.5396 - val_loss: 1.3795 - val_accuracy: 0.5277\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3385 - accuracy: 0.5407 - val_loss: 1.3791 - val_accuracy: 0.5247\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3369 - accuracy: 0.5396 - val_loss: 1.3777 - val_accuracy: 0.5270\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3360 - accuracy: 0.5429 - val_loss: 1.3769 - val_accuracy: 0.5267\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3341 - accuracy: 0.5431 - val_loss: 1.3761 - val_accuracy: 0.5270\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3365 - accuracy: 0.5426 - val_loss: 1.3763 - val_accuracy: 0.5277\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3323 - accuracy: 0.5433 - val_loss: 1.3756 - val_accuracy: 0.5280\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3319 - accuracy: 0.5421 - val_loss: 1.3745 - val_accuracy: 0.5263\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3299 - accuracy: 0.5439 - val_loss: 1.3738 - val_accuracy: 0.5270\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3307 - accuracy: 0.5417 - val_loss: 1.3730 - val_accuracy: 0.5280\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3298 - accuracy: 0.5407 - val_loss: 1.3725 - val_accuracy: 0.5290\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3281 - accuracy: 0.5440 - val_loss: 1.3719 - val_accuracy: 0.5290\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3264 - accuracy: 0.5460 - val_loss: 1.3707 - val_accuracy: 0.5297\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3259 - accuracy: 0.5435 - val_loss: 1.3702 - val_accuracy: 0.5307\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3245 - accuracy: 0.5474 - val_loss: 1.3698 - val_accuracy: 0.5290\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3267 - accuracy: 0.5444 - val_loss: 1.3684 - val_accuracy: 0.5290\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3267 - accuracy: 0.5447 - val_loss: 1.3685 - val_accuracy: 0.5310\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3229 - accuracy: 0.5465 - val_loss: 1.3679 - val_accuracy: 0.5300\n",
      "Epoch 79/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3225 - accuracy: 0.5457 - val_loss: 1.3676 - val_accuracy: 0.5303\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3260 - accuracy: 0.5445 - val_loss: 1.3664 - val_accuracy: 0.5313\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3222 - accuracy: 0.5466 - val_loss: 1.3660 - val_accuracy: 0.5303\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3206 - accuracy: 0.5456 - val_loss: 1.3649 - val_accuracy: 0.5313\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3220 - accuracy: 0.5466 - val_loss: 1.3646 - val_accuracy: 0.5337\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3184 - accuracy: 0.5495 - val_loss: 1.3639 - val_accuracy: 0.5310\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3223 - accuracy: 0.5467 - val_loss: 1.3638 - val_accuracy: 0.5320\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3206 - accuracy: 0.5481 - val_loss: 1.3630 - val_accuracy: 0.5327\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3195 - accuracy: 0.5476 - val_loss: 1.3627 - val_accuracy: 0.5337\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3181 - accuracy: 0.5476 - val_loss: 1.3625 - val_accuracy: 0.5313\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3180 - accuracy: 0.5460 - val_loss: 1.3620 - val_accuracy: 0.5333\n",
      "Epoch 90/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3161 - accuracy: 0.5487 - val_loss: 1.3616 - val_accuracy: 0.5323\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3179 - accuracy: 0.5457 - val_loss: 1.3605 - val_accuracy: 0.5313\n",
      "Epoch 92/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3142 - accuracy: 0.5465 - val_loss: 1.3602 - val_accuracy: 0.5350\n",
      "Epoch 93/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3172 - accuracy: 0.5483 - val_loss: 1.3595 - val_accuracy: 0.5353\n",
      "Epoch 94/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3138 - accuracy: 0.5474 - val_loss: 1.3589 - val_accuracy: 0.5343\n",
      "Epoch 95/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3159 - accuracy: 0.5463 - val_loss: 1.3589 - val_accuracy: 0.5353\n",
      "Epoch 96/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3162 - accuracy: 0.5490 - val_loss: 1.3584 - val_accuracy: 0.5327\n",
      "Epoch 97/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3128 - accuracy: 0.5489 - val_loss: 1.3579 - val_accuracy: 0.5330\n",
      "Epoch 98/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3112 - accuracy: 0.5487 - val_loss: 1.3570 - val_accuracy: 0.5363\n",
      "Epoch 99/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3128 - accuracy: 0.5490 - val_loss: 1.3560 - val_accuracy: 0.5347\n",
      "Epoch 100/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3111 - accuracy: 0.5493 - val_loss: 1.3563 - val_accuracy: 0.5353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5770b4adc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_train,\n",
    "           y_train,\n",
    "           epochs=100,\n",
    "           batch_size=128,\n",
    "           validation_data=(input_test, y_test)\n",
    "           , callbacks=[stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuCcwZzYmlYq"
   },
   "source": [
    "Dessa vez o Early stopping nao foi disparado, ou seja, foi executado todas das epocas.\n",
    "\n",
    "Resultados: \n",
    "\n",
    "1- O InverseTimeDecay foi o que mais demorou para convergir e em 100 epocas o maximo que ele chegou na val_loss foi 1.35. Foi o pior desempenho em relação aos outros para esse conjunto de dados. \n",
    "\n",
    "2-Tempo de execução individual a cada epoca foi parecido com a ExponencialDecay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HX2aw-Rnk1Q"
   },
   "source": [
    "Conclusão: Para esse conjunto de dados, a o melhor algoritmo foi o PiecewiseConstantDecay. Manteve a menor val_loss = 1.04 e convergiu mais rapido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3jKzmxKn3z4",
    "outputId": "c4682885-b070-4083-a234-6f406b318089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 7s 22ms/step - loss: 8.8188 - accuracy: 0.2963 - val_loss: 6.7996 - val_accuracy: 0.4510\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 5.3664 - accuracy: 0.5085 - val_loss: 4.1632 - val_accuracy: 0.5333\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 3.3038 - accuracy: 0.5575 - val_loss: 2.6697 - val_accuracy: 0.5543\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 2.2862 - accuracy: 0.5675 - val_loss: 2.0542 - val_accuracy: 0.5590\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.9064 - accuracy: 0.5672 - val_loss: 1.8326 - val_accuracy: 0.5550\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.7611 - accuracy: 0.5629 - val_loss: 1.7376 - val_accuracy: 0.5483\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6879 - accuracy: 0.5679 - val_loss: 1.6841 - val_accuracy: 0.5547\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6420 - accuracy: 0.5717 - val_loss: 1.6494 - val_accuracy: 0.5527\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6098 - accuracy: 0.5703 - val_loss: 1.6209 - val_accuracy: 0.5603\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.5843 - accuracy: 0.5759 - val_loss: 1.5977 - val_accuracy: 0.5623\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5643 - accuracy: 0.5785 - val_loss: 1.5819 - val_accuracy: 0.5613\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5481 - accuracy: 0.5810 - val_loss: 1.5712 - val_accuracy: 0.5620\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5331 - accuracy: 0.5809 - val_loss: 1.5517 - val_accuracy: 0.5720\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5217 - accuracy: 0.5820 - val_loss: 1.5451 - val_accuracy: 0.5657\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.5111 - accuracy: 0.5889 - val_loss: 1.5342 - val_accuracy: 0.5710\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5025 - accuracy: 0.5897 - val_loss: 1.5239 - val_accuracy: 0.5763\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4928 - accuracy: 0.5908 - val_loss: 1.5156 - val_accuracy: 0.5773\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4856 - accuracy: 0.5922 - val_loss: 1.5115 - val_accuracy: 0.5837\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.4771 - accuracy: 0.5923 - val_loss: 1.5043 - val_accuracy: 0.5743\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4691 - accuracy: 0.5974 - val_loss: 1.4981 - val_accuracy: 0.5820\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4633 - accuracy: 0.5948 - val_loss: 1.4906 - val_accuracy: 0.5793\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4606 - accuracy: 0.5953 - val_loss: 1.4875 - val_accuracy: 0.5847\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4543 - accuracy: 0.5936 - val_loss: 1.4864 - val_accuracy: 0.5883\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4492 - accuracy: 0.5993 - val_loss: 1.4804 - val_accuracy: 0.5860\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.4457 - accuracy: 0.6012 - val_loss: 1.4725 - val_accuracy: 0.5853\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4435 - accuracy: 0.6010 - val_loss: 1.4795 - val_accuracy: 0.5847\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4360 - accuracy: 0.6022 - val_loss: 1.4769 - val_accuracy: 0.5893\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4323 - accuracy: 0.6032 - val_loss: 1.4680 - val_accuracy: 0.5887\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 1.4291 - accuracy: 0.6052 - val_loss: 1.4633 - val_accuracy: 0.5930\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4276 - accuracy: 0.6063 - val_loss: 1.4558 - val_accuracy: 0.5927\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4233 - accuracy: 0.6048 - val_loss: 1.4509 - val_accuracy: 0.5910\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4186 - accuracy: 0.6069 - val_loss: 1.4534 - val_accuracy: 0.5900\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.4147 - accuracy: 0.6089 - val_loss: 1.4530 - val_accuracy: 0.5913\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.4146 - accuracy: 0.6075 - val_loss: 1.4431 - val_accuracy: 0.5937\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.4106 - accuracy: 0.6069 - val_loss: 1.4483 - val_accuracy: 0.5950\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.4100 - accuracy: 0.6066 - val_loss: 1.4407 - val_accuracy: 0.5893\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.4071 - accuracy: 0.6089 - val_loss: 1.4355 - val_accuracy: 0.6017\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.4046 - accuracy: 0.6081 - val_loss: 1.4424 - val_accuracy: 0.5927\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.4024 - accuracy: 0.6083 - val_loss: 1.4355 - val_accuracy: 0.5960\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.4005 - accuracy: 0.6100 - val_loss: 1.4391 - val_accuracy: 0.5890\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3979 - accuracy: 0.6110 - val_loss: 1.4339 - val_accuracy: 0.5963\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3963 - accuracy: 0.6106 - val_loss: 1.4380 - val_accuracy: 0.5943\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3944 - accuracy: 0.6128 - val_loss: 1.4388 - val_accuracy: 0.5963\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3925 - accuracy: 0.6116 - val_loss: 1.4284 - val_accuracy: 0.5887\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3908 - accuracy: 0.6117 - val_loss: 1.4225 - val_accuracy: 0.6020\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3882 - accuracy: 0.6138 - val_loss: 1.4241 - val_accuracy: 0.5953\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3849 - accuracy: 0.6138 - val_loss: 1.4176 - val_accuracy: 0.5997\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3834 - accuracy: 0.6140 - val_loss: 1.4180 - val_accuracy: 0.5947\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3828 - accuracy: 0.6149 - val_loss: 1.4270 - val_accuracy: 0.5973\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3836 - accuracy: 0.6142 - val_loss: 1.4235 - val_accuracy: 0.5980\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3791 - accuracy: 0.6147 - val_loss: 1.4126 - val_accuracy: 0.5987\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3760 - accuracy: 0.6177 - val_loss: 1.4126 - val_accuracy: 0.5993\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3762 - accuracy: 0.6163 - val_loss: 1.4173 - val_accuracy: 0.5983\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3754 - accuracy: 0.6160 - val_loss: 1.4070 - val_accuracy: 0.6007\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3748 - accuracy: 0.6165 - val_loss: 1.4162 - val_accuracy: 0.6023\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3740 - accuracy: 0.6161 - val_loss: 1.4139 - val_accuracy: 0.5983\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3718 - accuracy: 0.6164 - val_loss: 1.4101 - val_accuracy: 0.6003\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3695 - accuracy: 0.6186 - val_loss: 1.4146 - val_accuracy: 0.5987\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3685 - accuracy: 0.6166 - val_loss: 1.4014 - val_accuracy: 0.5980\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3668 - accuracy: 0.6198 - val_loss: 1.4064 - val_accuracy: 0.6020\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3636 - accuracy: 0.6189 - val_loss: 1.4109 - val_accuracy: 0.5957\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3660 - accuracy: 0.6172 - val_loss: 1.4116 - val_accuracy: 0.5987\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3646 - accuracy: 0.6166 - val_loss: 1.4061 - val_accuracy: 0.6080\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.3613 - accuracy: 0.6198 - val_loss: 1.4059 - val_accuracy: 0.6023\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3590 - accuracy: 0.6206 - val_loss: 1.4004 - val_accuracy: 0.5990\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.3603 - accuracy: 0.6190 - val_loss: 1.4067 - val_accuracy: 0.6027\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3586 - accuracy: 0.6191 - val_loss: 1.4030 - val_accuracy: 0.5980\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3583 - accuracy: 0.6225 - val_loss: 1.3960 - val_accuracy: 0.6047\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.3561 - accuracy: 0.6203 - val_loss: 1.3997 - val_accuracy: 0.6077\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3581 - accuracy: 0.6200 - val_loss: 1.3994 - val_accuracy: 0.6013\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3547 - accuracy: 0.6192 - val_loss: 1.3983 - val_accuracy: 0.5993\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3539 - accuracy: 0.6210 - val_loss: 1.4107 - val_accuracy: 0.5940\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3521 - accuracy: 0.6211 - val_loss: 1.3896 - val_accuracy: 0.6087\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.3525 - accuracy: 0.6192 - val_loss: 1.3980 - val_accuracy: 0.6023\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3513 - accuracy: 0.6223 - val_loss: 1.3947 - val_accuracy: 0.5997\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.3515 - accuracy: 0.6193 - val_loss: 1.3979 - val_accuracy: 0.6003\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3515 - accuracy: 0.6210 - val_loss: 1.3923 - val_accuracy: 0.6013\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3496 - accuracy: 0.6213 - val_loss: 1.3970 - val_accuracy: 0.5970\n",
      "Epoch 79/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3479 - accuracy: 0.6201 - val_loss: 1.3806 - val_accuracy: 0.6040\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3472 - accuracy: 0.6224 - val_loss: 1.3877 - val_accuracy: 0.6037\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3464 - accuracy: 0.6226 - val_loss: 1.3838 - val_accuracy: 0.6087\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3459 - accuracy: 0.6224 - val_loss: 1.3846 - val_accuracy: 0.6007\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.3425 - accuracy: 0.6247 - val_loss: 1.3852 - val_accuracy: 0.6033\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.3450 - accuracy: 0.6214 - val_loss: 1.3847 - val_accuracy: 0.6047\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3433 - accuracy: 0.6238 - val_loss: 1.3897 - val_accuracy: 0.6030\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.3428 - accuracy: 0.6212 - val_loss: 1.3937 - val_accuracy: 0.5973\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.3411 - accuracy: 0.6238 - val_loss: 1.3941 - val_accuracy: 0.6120\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3406 - accuracy: 0.6227 - val_loss: 1.3835 - val_accuracy: 0.6077\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.3394 - accuracy: 0.6256 - val_loss: 1.3869 - val_accuracy: 0.6033\n",
      "Epoch 89: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5770ad3d00>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model2.add(layer)\n",
    "\n",
    "for layer in model2.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(l=0.01)))\n",
    "model2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps = 10000,\n",
    "    decay_rate = 0.9)\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(input_train,\n",
    "           y_train,\n",
    "           epochs=100,\n",
    "           batch_size=128,\n",
    "           validation_data=(input_test, y_test)\n",
    "           , callbacks=[stopper])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agBJaT4GpQcN"
   },
   "source": [
    "Usei o mesmo modelo que usei o ExponencialDecay, só que dessa vez regularizando com L1 a penultima camada da rede.\n",
    "\n",
    "Resultado: \n",
    "\n",
    "1- O algoritmo ficou ligeiramente mais lento. \n",
    "\n",
    "2- val_loss convergiu para 1.38. Resultado muito pior que sem regularização. Lembrando que sem regularização val_loss = 1.04.\n",
    "\n",
    "3- Com regularização o modelo parou na epoca 89, sem regularização parou na epoca 55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSMI1Ez4pyo4",
    "outputId": "107ac7f0-59b8-4736-f9b3-e5acf5cfcaef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 7s 27ms/step - loss: 8.9227 - accuracy: 0.2500 - val_loss: 6.6682 - val_accuracy: 0.4413\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 5.3356 - accuracy: 0.4301 - val_loss: 4.0611 - val_accuracy: 0.5237\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 3.3003 - accuracy: 0.4943 - val_loss: 2.6443 - val_accuracy: 0.5450\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 2.3468 - accuracy: 0.5087 - val_loss: 2.0943 - val_accuracy: 0.5440\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 2.0209 - accuracy: 0.5079 - val_loss: 1.9102 - val_accuracy: 0.5437\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.8988 - accuracy: 0.5113 - val_loss: 1.8235 - val_accuracy: 0.5477\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.8403 - accuracy: 0.5100 - val_loss: 1.7745 - val_accuracy: 0.5513\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.8030 - accuracy: 0.5167 - val_loss: 1.7343 - val_accuracy: 0.5503\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.7728 - accuracy: 0.5197 - val_loss: 1.7061 - val_accuracy: 0.5617\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.7495 - accuracy: 0.5244 - val_loss: 1.6891 - val_accuracy: 0.5673\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.7337 - accuracy: 0.5267 - val_loss: 1.6780 - val_accuracy: 0.5657\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.7203 - accuracy: 0.5292 - val_loss: 1.6580 - val_accuracy: 0.5640\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.7069 - accuracy: 0.5333 - val_loss: 1.6494 - val_accuracy: 0.5713\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.6966 - accuracy: 0.5348 - val_loss: 1.6426 - val_accuracy: 0.5680\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6892 - accuracy: 0.5353 - val_loss: 1.6284 - val_accuracy: 0.5703\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.6783 - accuracy: 0.5358 - val_loss: 1.6192 - val_accuracy: 0.5760\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.6754 - accuracy: 0.5387 - val_loss: 1.6163 - val_accuracy: 0.5780\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6688 - accuracy: 0.5392 - val_loss: 1.6084 - val_accuracy: 0.5803\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6653 - accuracy: 0.5367 - val_loss: 1.6024 - val_accuracy: 0.5813\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6605 - accuracy: 0.5408 - val_loss: 1.5989 - val_accuracy: 0.5750\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.6525 - accuracy: 0.5434 - val_loss: 1.5984 - val_accuracy: 0.5773\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6462 - accuracy: 0.5443 - val_loss: 1.5928 - val_accuracy: 0.5803\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.6456 - accuracy: 0.5433 - val_loss: 1.5881 - val_accuracy: 0.5830\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6474 - accuracy: 0.5446 - val_loss: 1.5830 - val_accuracy: 0.5813\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6394 - accuracy: 0.5455 - val_loss: 1.5790 - val_accuracy: 0.5823\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 6s 23ms/step - loss: 1.6412 - accuracy: 0.5419 - val_loss: 1.5711 - val_accuracy: 0.5873\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6382 - accuracy: 0.5412 - val_loss: 1.5751 - val_accuracy: 0.5807\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6336 - accuracy: 0.5483 - val_loss: 1.5718 - val_accuracy: 0.5807\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6294 - accuracy: 0.5472 - val_loss: 1.5695 - val_accuracy: 0.5803\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6239 - accuracy: 0.5467 - val_loss: 1.5664 - val_accuracy: 0.5833\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6276 - accuracy: 0.5472 - val_loss: 1.5566 - val_accuracy: 0.5913\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6264 - accuracy: 0.5495 - val_loss: 1.5588 - val_accuracy: 0.5827\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6214 - accuracy: 0.5462 - val_loss: 1.5585 - val_accuracy: 0.5780\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6206 - accuracy: 0.5481 - val_loss: 1.5631 - val_accuracy: 0.5847\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6230 - accuracy: 0.5472 - val_loss: 1.5548 - val_accuracy: 0.5880\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6222 - accuracy: 0.5440 - val_loss: 1.5543 - val_accuracy: 0.5787\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6176 - accuracy: 0.5469 - val_loss: 1.5616 - val_accuracy: 0.5803\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6146 - accuracy: 0.5490 - val_loss: 1.5525 - val_accuracy: 0.5930\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6150 - accuracy: 0.5466 - val_loss: 1.5489 - val_accuracy: 0.5817\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6138 - accuracy: 0.5476 - val_loss: 1.5552 - val_accuracy: 0.5797\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6179 - accuracy: 0.5465 - val_loss: 1.5492 - val_accuracy: 0.5837\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6153 - accuracy: 0.5462 - val_loss: 1.5472 - val_accuracy: 0.5830\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6118 - accuracy: 0.5506 - val_loss: 1.5404 - val_accuracy: 0.5883\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6090 - accuracy: 0.5498 - val_loss: 1.5456 - val_accuracy: 0.5820\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6110 - accuracy: 0.5477 - val_loss: 1.5326 - val_accuracy: 0.5910\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6108 - accuracy: 0.5491 - val_loss: 1.5418 - val_accuracy: 0.5840\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6066 - accuracy: 0.5491 - val_loss: 1.5403 - val_accuracy: 0.5893\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6101 - accuracy: 0.5485 - val_loss: 1.5423 - val_accuracy: 0.5817\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6023 - accuracy: 0.5517 - val_loss: 1.5325 - val_accuracy: 0.5763\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6003 - accuracy: 0.5512 - val_loss: 1.5305 - val_accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6112 - accuracy: 0.5463 - val_loss: 1.5450 - val_accuracy: 0.5860\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6026 - accuracy: 0.5502 - val_loss: 1.5368 - val_accuracy: 0.5797\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5999 - accuracy: 0.5500 - val_loss: 1.5266 - val_accuracy: 0.5923\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5995 - accuracy: 0.5484 - val_loss: 1.5351 - val_accuracy: 0.5927\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.6001 - accuracy: 0.5481 - val_loss: 1.5344 - val_accuracy: 0.5793\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6007 - accuracy: 0.5488 - val_loss: 1.5228 - val_accuracy: 0.5877\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5979 - accuracy: 0.5512 - val_loss: 1.5388 - val_accuracy: 0.5870\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.6007 - accuracy: 0.5471 - val_loss: 1.5333 - val_accuracy: 0.5807\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.6005 - accuracy: 0.5482 - val_loss: 1.5293 - val_accuracy: 0.5973\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5997 - accuracy: 0.5500 - val_loss: 1.5309 - val_accuracy: 0.5923\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5918 - accuracy: 0.5493 - val_loss: 1.5301 - val_accuracy: 0.5850\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5911 - accuracy: 0.5501 - val_loss: 1.5280 - val_accuracy: 0.5947\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5987 - accuracy: 0.5481 - val_loss: 1.5367 - val_accuracy: 0.5903\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5937 - accuracy: 0.5484 - val_loss: 1.5253 - val_accuracy: 0.5893\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5946 - accuracy: 0.5500 - val_loss: 1.5318 - val_accuracy: 0.5837\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5968 - accuracy: 0.5489 - val_loss: 1.5169 - val_accuracy: 0.5960\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5967 - accuracy: 0.5458 - val_loss: 1.5186 - val_accuracy: 0.5810\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5930 - accuracy: 0.5523 - val_loss: 1.5178 - val_accuracy: 0.5860\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5933 - accuracy: 0.5463 - val_loss: 1.5365 - val_accuracy: 0.5890\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5908 - accuracy: 0.5512 - val_loss: 1.5173 - val_accuracy: 0.5933\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5875 - accuracy: 0.5507 - val_loss: 1.5230 - val_accuracy: 0.5897\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5898 - accuracy: 0.5482 - val_loss: 1.5090 - val_accuracy: 0.5960\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5927 - accuracy: 0.5517 - val_loss: 1.5113 - val_accuracy: 0.5953\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5843 - accuracy: 0.5541 - val_loss: 1.5175 - val_accuracy: 0.5877\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5952 - accuracy: 0.5487 - val_loss: 1.5165 - val_accuracy: 0.5853\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5899 - accuracy: 0.5519 - val_loss: 1.5133 - val_accuracy: 0.5887\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5908 - accuracy: 0.5499 - val_loss: 1.5072 - val_accuracy: 0.5873\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5865 - accuracy: 0.5495 - val_loss: 1.5047 - val_accuracy: 0.5833\n",
      "Epoch 79/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5874 - accuracy: 0.5541 - val_loss: 1.5060 - val_accuracy: 0.5957\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5875 - accuracy: 0.5507 - val_loss: 1.5106 - val_accuracy: 0.5940\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5869 - accuracy: 0.5520 - val_loss: 1.5114 - val_accuracy: 0.5967\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5894 - accuracy: 0.5539 - val_loss: 1.5043 - val_accuracy: 0.5937\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5822 - accuracy: 0.5528 - val_loss: 1.5136 - val_accuracy: 0.5863\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5886 - accuracy: 0.5501 - val_loss: 1.5127 - val_accuracy: 0.5937\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 1.5788 - accuracy: 0.5520 - val_loss: 1.5134 - val_accuracy: 0.5910\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.5852 - accuracy: 0.5494 - val_loss: 1.5094 - val_accuracy: 0.5813\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5852 - accuracy: 0.5511 - val_loss: 1.5075 - val_accuracy: 0.5977\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.5796 - accuracy: 0.5492 - val_loss: 1.5153 - val_accuracy: 0.5863\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.5809 - accuracy: 0.5503 - val_loss: 1.5144 - val_accuracy: 0.5807\n",
      "Epoch 90/100\n",
      "235/235 [==============================] - 5s 23ms/step - loss: 1.5866 - accuracy: 0.5485 - val_loss: 1.5061 - val_accuracy: 0.5910\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 1.5786 - accuracy: 0.5500 - val_loss: 1.5054 - val_accuracy: 0.5843\n",
      "Epoch 92/100\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 1.5832 - accuracy: 0.5507 - val_loss: 1.5097 - val_accuracy: 0.5780\n",
      "Epoch 92: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f577083ddf0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = keras.Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model3.add(layer)\n",
    "\n",
    "for layer in model3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(tf.keras.layers.BatchNormalization())\n",
    "model3.add(tf.keras.layers.Dropout(.5, input_shape=(2,)))\n",
    "model3.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(l=0.01)))\n",
    "model3.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps = 10000,\n",
    "    decay_rate = 0.9)\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model3.fit(input_train,\n",
    "           y_train,\n",
    "           epochs=100,\n",
    "           batch_size=128,\n",
    "           validation_data=(input_test, y_test)\n",
    "           , callbacks=[stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8owCAxepqykn"
   },
   "source": [
    "Como ultimo teste. Usei a rede anterior, com regularização, só que dessa vez adicionei um dropout de 0.5\n",
    "ExponencialDecay com regularização e dropout(1) deve o pior desempenho de todas as redes. Comparado com a ExponencialDecay sem regularização e sem dropout(2): \n",
    "\n",
    "1- (1) val_loss = 1.51, (2) val_loss = 1.04. \n",
    "\n",
    "2- (1) epoca = 92, (2) epoca = 55, ou seja, o (1) demorou muito mais para convergir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqc4kvI-s4go"
   },
   "source": [
    "Conclusão: Melhor rede foi do algoritmo PiecewiseConstantDecay. Convergiu mais rapido e teve o menor val_loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sirPhQl1uSLR"
   },
   "source": [
    "Qualquer duvida, estou a disposição. Obrigado."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
